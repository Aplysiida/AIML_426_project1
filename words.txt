Project 1: 

The solution for the knapsack problem is a sequence of items that were selected for the knapsack which also keeps track of each itemâ€™s value and weight. The order of the selected items does not matter, but constraints such as the maximum weight constraint need to be easily defined into the solution. 

Individual Representation: 

For this GA the chromosome is represented through a bit vector whose length is the number of possible items to choose from and each element represents a possible item to select for the knapsack. For decoding the chromosome into a solution for each bit, 1 represents selecting the item while 0 represents not selecting the item.  

Since the order of the items picked up does not matter, the order of items in the chromosome can be the same as the order of items in the weight and values list. This means that the value and weight can be easily obtained by looking at the index of the chosen item in the chromosome in the values and weights lists. The crossover and mutation genetic operators can easily be applied to the chromosome and the solution can be decoded no matter how much the chromosome has changed. 

Fitness Function: 

The fitness of the knapsack solutions is measured in the maximum value of items chosen while also satisfying the maximum weight constraint. The evaluate the maximum value of the knapsack solution, the sum of values of the items selected is calculated. To implement the constraint a penalty variable is introduced, the formula that is used to evaluate when to use the penalty variable is %insert formula here%. So, when the total weight of the solution is greater than the maximum weight, the penalty is applied. The value of the penalty variable is set to 10.0 to heavily discourage solutions that violate the constraint by setting them with very low fitness. 

%talk about how setting penalty variable too high got best solutions caught up in local optima 

%show different graphs of convergence curve with different penalty value 

Alpha = 1 

Alpha = 3 

Alpha = 5 

Since the fitness values are used in calculating probabilities, fitness >= 0 must be satisfied. 

Genetic Operators 

Selection 

%might change selection schused 

A fitness-proportional/roulette wheel selection scheme is used for this problem. When implementing the roulette wheel selection, the probability: %insert probability formula here% is calculated. A higher probability means a higher chance of being selected either for elitism or for parents. This was chosen since a roulette wheel selection is simple to implement and it encourages selecting high fitness solutions through having a larger probability of being chosen. This is also where the maximum weight constraint is implemented through. 

Crossover 

One-point crossover was used for the crossover genetic operator, since the order of the items selected does not matter, no constraints need to be defined for the split position of the crossover. 

Mutation 

Local search mutation was used for the mutation genetic operator. Instead of randomly selecting a bit and flipping the value, the flip position with the largest increase in value is selected. This does increase the computation requirement since instead of O(1) complexity it is now O(M) complexity requiring to now iterate through each possible flip, but also increase chance of improving population each generation. 

Selection 

The proportional fitness/roulette wheel selection was chosen since it is simple to implement through calculating the probability of being selected for each instance as $$. This probability calculation requires that the fitness value is never negative. 

Part 3:
Individual rep:
Each individual needs to keep track of classification performance and ratio of selected features. The bit vector is used again for this problem where the classification performance is determined by using a wrapper classifier on the features subset and the ratio of selected features is determined by #insert the math formula (number of 1s in bit vector)/(total length of bit vector). For individual generation, since there are no constraints it uses the same generation as used for Part 2.
Wrapper-based fitness function:
KNN is used for the wrapper fitness used in the classification performance since it has a good balance of accuracy and speed.
Genetic Operators:
Since this problem uses the same instance representation and problem used for Part 2, the same genetic operators are used for this problem.


Part 4:
Individual are represented using a tree of functions and terminals. These trees are generated using the ramp-half-and-half method where half of the trees are generated using the grow method and the other half generated using the full method to encourage diversity in the population.
The minimum and maximum height of the trees generated initially is 1 and 4. 1 is chosen since having trees with smaller depth is not useful. 4 is chosen since depth 3 a full tree will have 15 nodes, thus is already a complicated tree. 
For the function and terminal sets.

Operators and Parameters chosen:
For the genetic operators of selection, crossover and mutation: tournament selection, 1-point crossover and uniform mutation were chosen. 
Tournament selection was chosen because it provides a balance between exploration of the tree instance space and optimization. The tournament size was set to 5 to achieve the exploration/optimzation balance as well.
1-point crossover was chosen since swapping one subtree from each parent creates a unique enough child.
Since there is no focus needed for creating more complicated trees or creating simpler trees, for mutation an uniform chance of mutating into a new subtree or just a terminal node is chosen. The possible subtrees generated in the mutation have sizes defined up to depth of 1, since having larger subtrees could make the trees too complicated in a few iterations.
To avoid very complicated trees which can be slow to evaluate and expensive in memory, the maximum depth was defined to be at 17.

Fitness:
For the fitness cases, the range defined for the x values needs to small enough such that the $f(x <= 0)$ values is not so large that the GP might define the $f(x > 0)$ values as smooth in comparison. 30 x valuesevenly spread out in the range [-6.0, 15.0] were generated into the dataset to be used by the GP. More x values were defined when $x > 0$ to capture the #1/x effect on the formula and to capture the sin() pattern.
For the fitness function, it is defined as the mean square error between the solution generated function and the problem defined function using the x values defined for the fitness cases. The aim of the GP is to minimising this mean square error value.
