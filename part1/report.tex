\documentclass{article}

\title{AIML426 Project 1 Report}
\date{}

\begin{document}
\maketitle
\section*{Part 1}
\subsection*{Individual Representation}
For this GA the chromosome is represented through a bit vector whose length is the number of possible items to choose from, each element represents a possible item to select for the knapsack. For decoding the chromosome into a solution, for each bit, 1 represents selecting the item while 0 represents not selecting the item.  \par
\noindent Since the order of the items picked up does not matter, the order of items in the chromosome can be the same as the order of items in the weight and values list. This means that the value and weight can be easily obtained by looking at the index of the chosen item in the chromosome in the values and weights lists. The crossover and mutation genetic operators can easily be applied to the chromosome and the solution can be decoded no matter how much the chromosome has changed. \par
\subsubsection*{Individual Generation}
\noindent At first for individual generation a random bit list of range of items length was generated. This worked for the first two datasets, but a problem that was encountered with the third dataset is that it has a large range of items to select, but has a small weight constraint. Thus the majority of individual generated violated the constraint by a lot and the initial population have 0 total fitness and never improve for a long time. \par
\noindent To avoid this the constraint was implemented into the individual generation for the initial population so that all the individuals satisfied the constraint. This was done by starting with a 0 chromosome and setting bits to 1 randomly in the chromosome until the weight constraint has been reached. This will still give a diverse range of individuals while setting the initial population fitness to above 0 and thus having a greater chance of improving the overall population fitness before convergence. \par
\subsection*{Fitness Function}
The fitness of the knapsack solutions is measured in the maximum value of items chosen while also satisfying the maximum weight constraint. The evaluate the maximum value of the knapsack solution, the sum of values of the items selected is calculated. To implement the constraint a penalty variable is introduced, the formula that is used to evaluate when to use the penalty variable for individual of $M$ bits is 
\begin{center} 
$max(0, \sum_{i=1}^{M}v_i - \alpha *max(0,\sum_{i=1}^{M}w_i)-max weight)$. 
\end{center}
So, when the total weight of the solution is greater than the maximum weight, the penalty is applied. A minimum fitness of 0 is set since negative fitness does not work with probabilities.
\subsection*{Genetic Operators}
\subsubsection*{Crossover}
One-point crossover was used for the crossover genetic operator, since the order of the items selected does not matter, no constraints need to be defined for the split position of the crossover. 
\subsubsection*{Mutation}
\subsection*{GA Parameter Values}
%Generation Number and Population Size
\subsubsection*{Number of Generations and Population Size}
To achieve close to the optimal value, different population sizes and generation numbers was tuned for each dataset. For the first and second dataset, population size of 50 and generation number of 100 was set since the dataset is small, thus reaching the optimal value can be quicker. For the third dataset, because of the strict constraint, 100 was set for the population size since there are less individuals that will satisfy the constraint of this dataset. This does mean that running the GA for this dataset is very time consuming so the population size did not increase beyond 100.
%Generations until convergence
\subsubsection*{Iterations Until Convergence}
This was set to 5 so that convergence only occurs when it is obvious that the fitness, measured as the average of the top 5 best individuals, no longer changes each generation. Thus saving computation time with the GA avoiding a large number of iterations with no effect. If set too large, the computation time saved becomes small.
%Selection used
\subsubsection*{Selection Scheme Used}
A fitness-proportional/roulette wheel selection scheme is used for this problem. When implementing the roulette wheel selection, the probability: $\frac{fitness(i)}{pop\_fitness}$ is calculated. A higher probability means a higher chance of being selected either for elitism or for parents. This was chosen since a roulette wheel selection is simple to implement and it encourages selecting high fitness solutions through having a larger probability of being chosen. This is also where the maximum weight constraint is implemented through. 
%Alpha value
\subsubsection*{Alpha/Penalty Coefficient}
The penalty coefficient/alpha variable needs to be tuned to the dataset so it is strong enough to generally ignore solutions that violate constraint when selecting individuals, but small enough to still acknowledge solutions that only slightly violate constraint but have very good value sums. \par
\noindent For the first dataset, an alpha value of 2 was selected since the was a small range of items to select from. \par
\noindent For the second dataset, there was a greater range of items to select from so the alpha value was higher to 3. \par
\noindent For the third dataset, while there was a much greater range of items to select and the weight constraint was very small. Thus the alpha value was set very high to make sure that the best individual does not violate the constraint. \par
%Crossover, Mutation, Elitism rates
\subsubsection*{Crossover, Mutation and Elitism Rates}
For both the first and second dataset, the parameter rates used are: 
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	Elitism Rate: & 0.03 \\
	\hline
	Crossover Rate: & 1.0 \\
	\hline
	Mutation Rate: & 0.3 \\
	\hline
\end{tabular}
\end{center}
For the third dataset, a much higher elitism rate was used to encourage more fit individuals for the next generation since fit individuals means that they do not violate the constraint which is difficult to achieve. The parameter rates used are:
\begin{center}
\begin{tabular}{|c|c|}
	\hline
	Elitism Rate: & 0.1 \\
	\hline
	Crossover Rate: & 1.0 \\
	\hline
	Mutation Rate:	& 0.3 \\
	\hline
\end{tabular}
\end{center}
\subsection*{Results}
%show tables here
%show best instance for the 5 seeds and datasets here
\subsubsection*{Discussion}
%talk about how slow dataset3 was since there were a large range of items to select, how it was very difficult to satisfy constraint for this dataset, though best individual always satisfied constraint
\subsubsection*{Conclusion}
%GA can easily identify optimal solution for small knapsack problems, but become time consuming for larger problems
%GA struggles with satisfying very strict constraints thus shown with some best averages being above the optimal value, which means some of the best individuals slightly violate the constraint even with a very high penalty coefficient set. Thus the very high penalty constraint. This could be fixed by implem
\section*{Part 2}

\end{document}